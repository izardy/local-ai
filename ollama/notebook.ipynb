{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HF Model into GGUF File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"unsloth/Llama-3.2-1B-bnb-4bit\"  # Replace with the ID of the model you want to download\n",
    "snapshot_download(repo_id=model_id, local_dir=\"quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone llama.cpp repo\n",
    "# !git clone https://github.com/ggerganov/llama.cpp\n",
    "# !pip install -r llama.cpp/requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Conversion Script (Model to GGUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./llama.cpp/convert_hf_to_gguf.py ./quantized --outfile output_file.gguf --outtype auto\n",
    "\n",
    "#llama.cpp options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR download GGUF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have git-lfs installed (https://git-lfs.com)\n",
    "# !git lfs install\n",
    "# !git clone https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF\n",
    "# !git clone https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-GGUF\n",
    "\n",
    "# via linux/mac download\n",
    "#!wget https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-GGUF/resolve/main/Llama-3.3-70B-Instruct-Q2_K.gguf?download=true -O Llama-3.3-70B-Instruct-Q2_K.gguf\n",
    "\n",
    "# via window download\n",
    "# the following is to download quantized (4bits) llama3.2-11bill from leafspark repo in gguf format\n",
    "!curl -L -O \"https://huggingface.co/leafspark/Llama-3.2-11B-Vision-Instruct-GGUF/resolve/main/Llama-3.2-11B-Vision-Instruct.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check existing Modelfile for specific LLM\n",
    "    > Check install model in Ollama\n",
    "    <small>\n",
    "    ```\n",
    "    ollama ls\n",
    "    ```\n",
    "    </small>\n",
    "\n",
    "    > Check Modelfile\n",
    "    <small>\n",
    "    ```\n",
    "    ollama show --modelfile modelname\n",
    "    ```\n",
    "    </small>\n",
    "- Example of Ollama Modelfile structure\n",
    "\n",
    "    > Specify the base model\n",
    "    <small>\n",
    "    ```\n",
    "    FROM llama2\n",
    "    ```\n",
    "    </small>\n",
    "    \n",
    "    > Configure model parameters\n",
    "    <small>\n",
    "    ```\n",
    "    PARAMETER temperature 0.7\n",
    "    PARAMETER top_k 40\n",
    "    PARAMETER top_p 0.9\n",
    "    ```\n",
    "    </small>\n",
    "    \n",
    "    > Define the template for input prompts\n",
    "    <small>\n",
    "    ```\n",
    "    TEMPLATE \"\"\"\n",
    "    USER: {{.Prompt}}\n",
    "    ASSISTANT: Let me help you with that.\n",
    "    \"\"\"\n",
    "    ```\n",
    "    </small>\n",
    "    \n",
    "    > Set the system message that defines the AI's behavior\n",
    "    <small>\n",
    "    ```\n",
    "    SYSTEM \"\"\"\n",
    "    You are a helpful and knowledgeable assistant who specializes in explaining technical concepts clearly and concisely. Please provide accurate and practical information while maintaining a professional tone.\n",
    "    \"\"\"\n",
    "    ```\n",
    "    </small>\n",
    "- Example of Modelfile (paste the following code in a non extension file name Modelfile)\n",
    "    > Llama-3.2\n",
    "    <small>\n",
    "    ```\n",
    "    # Modelfile\n",
    "    FROM \"Llama-3.2-11B-Vision-Instruct.Q4_K_M.gguf\" # This specifies that the model is based on Meta's LLaMA 3 70B model (quantized version)\n",
    "    TEMPLATE \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "    Cutting Knowledge Date: December 2023\n",
    "\n",
    "    {{ if .System }}{{ .System }}\n",
    "    {{- end }}\n",
    "    {{- if .Tools }}When you receive a tool call response, use the output to format an answer to the orginal user question.\n",
    "\n",
    "    You are a helpful assistant with tool calling capabilities.\n",
    "    {{- end }}<|eot_id|>\n",
    "    {{- range $i, $_ := .Messages }}\n",
    "    {{- $last := eq (len (slice $.Messages $i)) 1 }}\n",
    "    {{- if eq .Role \"user\" }}<|start_header_id|>user<|end_header_id|>\n",
    "    {{- if and $.Tools $last }}\n",
    "\n",
    "    Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
    "\n",
    "    Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\n",
    "\n",
    "    {{ range $.Tools }}\n",
    "    {{- . }}\n",
    "    {{ end }}\n",
    "    {{ .Content }}<|eot_id|>\n",
    "    {{- else }}\n",
    "\n",
    "    {{ .Content }}<|eot_id|>\n",
    "    {{- end }}{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "    {{ end }}\n",
    "    {{- else if eq .Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\n",
    "    {{- if .ToolCalls }}\n",
    "    {{ range .ToolCalls }}\n",
    "    {\"name\": \"{{ .Function.Name }}\", \"parameters\": {{ .Function.Arguments }}}{{ end }}\n",
    "    {{- else }}\n",
    "\n",
    "    {{ .Content }}\n",
    "    {{- end }}{{ if not $last }}<|eot_id|>{{ end }}\n",
    "    {{- else if eq .Role \"tool\" }}<|start_header_id|>ipython<|end_header_id|>\n",
    "\n",
    "    {{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "    {{ end }}\n",
    "    {{- end }}\n",
    "    {{- end }}\"\"\"\n",
    "    PARAMETER stop <|start_header_id|>\n",
    "    PARAMETER stop <|end_header_id|>\n",
    "    PARAMETER stop <|eot_id|>\n",
    "    ```\n",
    "    </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ollama Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama create llama3.2-q4 -f Modelfile\n",
    "\n",
    "'''\n",
    "Supported Quantizations\n",
    "q4_0\n",
    "q4_1\n",
    "q5_0\n",
    "q5_1\n",
    "q8_0\n",
    "\n",
    "K-means Quantizations\n",
    "q3_K_S\n",
    "q3_K_M\n",
    "q3_K_L\n",
    "q4_K_S\n",
    "q4_K_M\n",
    "q5_K_S\n",
    "q5_K_M\n",
    "q6_K\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
