{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert HF Model into GGUF File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"unsloth/Llama-3.2-1B-bnb-4bit\"  # Replace with the ID of the model you want to download\n",
    "snapshot_download(repo_id=model_id, local_dir=\"quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone llama.cpp repo\n",
    "# !git clone https://github.com/ggerganov/llama.cpp\n",
    "# !pip install -r llama.cpp/requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Conversion Script (Model to GGUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./llama.cpp/convert_hf_to_gguf.py ./quantized --outfile output_file.gguf --outtype auto\n",
    "\n",
    "#llama.cpp options\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR download GGUF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have git-lfs installed (https://git-lfs.com)\n",
    "# !git lfs install\n",
    "!git clone https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modelfile (paste the following code in a non extension file name Modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelfile\n",
    "FROM \"./Llama-3.2-1B-Instruct-GGUF/Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n",
    "TEMPLATE \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "\n",
    "{{ if .System }}{{ .System }}\n",
    "{{- end }}\n",
    "{{- if .Tools }}When you receive a tool call response, use the output to format an answer to the orginal user question.\n",
    "\n",
    "You are a helpful assistant with tool calling capabilities.\n",
    "{{- end }}<|eot_id|>\n",
    "{{- range $i, $_ := .Messages }}\n",
    "{{- $last := eq (len (slice $.Messages $i)) 1 }}\n",
    "{{- if eq .Role \"user\" }}<|start_header_id|>user<|end_header_id|>\n",
    "{{- if and $.Tools $last }}\n",
    "\n",
    "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
    "\n",
    "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\n",
    "\n",
    "{{ range $.Tools }}\n",
    "{{- . }}\n",
    "{{ end }}\n",
    "{{ .Content }}<|eot_id|>\n",
    "{{- else }}\n",
    "\n",
    "{{ .Content }}<|eot_id|>\n",
    "{{- end }}{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ end }}\n",
    "{{- else if eq .Role \"assistant\" }}<|start_header_id|>assistant<|end_header_id|>\n",
    "{{- if .ToolCalls }}\n",
    "{{ range .ToolCalls }}\n",
    "{\"name\": \"{{ .Function.Name }}\", \"parameters\": {{ .Function.Arguments }}}{{ end }}\n",
    "{{- else }}\n",
    "\n",
    "{{ .Content }}\n",
    "{{- end }}{{ if not $last }}<|eot_id|>{{ end }}\n",
    "{{- else if eq .Role \"tool\" }}<|start_header_id|>ipython<|end_header_id|>\n",
    "\n",
    "{{ .Content }}<|eot_id|>{{ if $last }}<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{{ end }}\n",
    "{{- end }}\n",
    "{{- end }}\"\"\"\n",
    "PARAMETER stop <|start_header_id|>\n",
    "PARAMETER stop <|end_header_id|>\n",
    "PARAMETER stop <|eot_id|>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ollama Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25ltransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 0% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 1% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 3% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 7% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 9% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 13% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 17% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 19% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 23% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 27% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 29% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 33% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 37% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 39% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 43% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 47% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 49% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 54% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 58% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 60% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 64% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 68% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 72% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 74% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 78% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 80% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 84% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 88% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 90% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 94% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 98% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data 100% \n",
      "using existing layer sha256:29085f34fffc0fae853b3952418d691a3a5c039f8b3af6628c39dc86ae2a8d89 \n",
      "creating new layer sha256:f2c2dcc20dde5f8d5c09ed76abbf794b1d566104bf81c3e71e5514c729ba6f37 \n",
      "using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb \n",
      "creating new layer sha256:4e158c72e5983738ff3374be54cf6638ddd9f31c868ce9899e77987e2c22f16e \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSupported Quantizations\\nq4_0\\nq4_1\\nq5_0\\nq5_1\\nq8_0\\n\\nK-means Quantizations\\nq3_K_S\\nq3_K_M\\nq3_K_L\\nq4_K_S\\nq4_K_M\\nq5_K_S\\nq5_K_M\\nq6_K\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ollama create llama3.2-q4 -f Modelfile\n",
    "\n",
    "'''\n",
    "Supported Quantizations\n",
    "q4_0\n",
    "q4_1\n",
    "q5_0\n",
    "q5_1\n",
    "q8_0\n",
    "\n",
    "K-means Quantizations\n",
    "q3_K_S\n",
    "q3_K_M\n",
    "q3_K_L\n",
    "q4_K_S\n",
    "q4_K_M\n",
    "q5_K_S\n",
    "q5_K_M\n",
    "q6_K\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
